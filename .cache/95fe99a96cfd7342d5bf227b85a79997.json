{
  "full_text": "Regression\nTushar B. Kute,\nhttp://tusharkute.com\n\nRegression?\n• Regression analysis is a statistical method that \nhelps us to analyse and understand the \nrelationship between two or more variables of \ninterest. \n• The process that is adapted to perform \nregression analysis helps to understand which \nfactors are important, which factors can be \nignored and how they are influencing each \nother.\n\nRegression?\n• Introduction, types of regression. Simple \nregression- Types, Making predictions, Cost \nfunction, Gradient descent, Training, Model \nevaluation.\n• Multivariable regression : Growing complexity, \nNormalization, Making predictions, Initialize \nweights, Cost function, Gradient descent, \nSimplifying with matrices, Bias term, Model \nevaluation\nTaken From-\nhttps://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html\n\nRegression?\n• For the regression analysis is be a successful \nmethod, we understand the following terms:\n– Dependent Variable: This is the variable that \nwe are trying to understand or forecast.\n– Independent Variable: These are factors that \ninfluence the analysis or target variable and \nprovide us with information regarding the \nrelationship of the variables with the target \nvariable.\n\nExample:\n\nExample:\n\nRegression \n• In regression, we normally have one dependent variable \nand one or more independent variables. \n• Here we try to “regress” the value of dependent \nvariable “Y” with the help of the independent variables. \n• In other words, we are trying to understand, how does \nthe value of ‘Y’ change w.r.t change in ‘X’.\n\nUses of Regression\n• Regression analysis is used for prediction and forecasting. This \nhas a substantial overlap to the field of machine learning. This \nstatistical method is used across different industries such as,\n– Financial Industry- Understand the trend in the stock prices, \nforecast the prices, evaluate risks in the insurance domain\n– Marketing- Understand the effectiveness of market \ncampaigns, forecast pricing and sales of the product. \n– Manufacturing- Evaluate the relationship of variables that \ndetermine to define a better engine to provide better \nperformance\n– Medicine- Forecast the different combination of medicines \nto prepare generic medicines for diseases.\n\nTerminologies\n• Outliers \n– Suppose there is an observation in the \ndataset that has a very high or very low value \nas compared to the other observations in the \ndata, i.e. it does not belong to the population, \nsuch an observation is called an outlier. \n– In simple words, it is an extreme value. An \noutlier is a problem because many times it \nhampers the results we get.\n\nTerminologies\n• Multicollinearity\n– When the independent variables are highly \ncorrelated to each other, then the variables are \nsaid to be multicollinear. \n– Many types of regression techniques assume \nmulticollinearity should not be present in the \ndataset. \n– It is because it causes problems in ranking \nvariables based on its importance, or it makes the \njob difficult in selecting the most important \nindependent variable.\n\nTerminologies\n• Heteroscedasticity\n– When the variation between the target variable and the \nindependent variable is not constant, it is called \nheteroscedasticity. \n– Example-As one’s income increases, the variability of \nfood consumption will increase. \n– A poorer person will spend a rather constant amount by \nalways eating inexpensive food; a wealthier person may \noccasionally buy inexpensive food and at other times, \neat expensive meals. \n– Those with higher incomes display a greater variability of \nfood consumption.\n\nTerminologies\n• When we use unnecessary explanatory \nvariables, it might lead to overfitting. \n• Overfitting means that our algorithm works \nwell on the training set but is unable to perform \nbetter on the test sets. It is also known as a \nproblem of high variance.\n• When our algorithm works so poorly that it is \nunable to fit even a training set well, then it is \nsaid to underfit the data. It is also known as a \nproblem of high bias.\n\nTerminologies\n\nTypes of Regression\n• Linear Regression\n• Multiple Regression\n• Logistic Regression\n• Polynomial Regression\n• Regularized Models\n– Ridge Regression\n– Lasso Regression\n– ElasticNet Regression\n• Outlier Based Model\n– RANSAC \n\nLinear Regression\n• The simplest of all regression types is Linear Regression \nwhere it tries to establish relationships between \nIndependent and Dependent variables. \n• The Dependent variable considered here is always a \ncontinuous variable.\n• Linear Regression is a predictive model used for finding \nthe linear relationship between a dependent variable \nand one or more independent variables.\n\nLinear Regression\n• Here, ‘Y’ is our dependent variable, which is a \ncontinuous numerical and we are trying to understand \nhow does ‘Y’ change with ‘X’.\n• So, if we are supposed to answer, the above question of \n“What will be the GRE score of the student, if his CCGPA \nis 8.32?” our go to option should be linear regression.\n\nSimple Linear Regression\n• As the model is used to predict the dependent \nvariable, the relationship between the variables can be \nwritten in the below format.\nYi = β0 + β1Xi +εi\n• Where,\n– Yi – Dependent variable\n– β0 — Intercept\n– β1 – Slope Coefficient\n– Xi – Independent Variable\n– εi – Random Error Term\n\nSimple Linear Regression\n• The main factor that is considered as part of Regression \nanalysis is understanding the variance between the variables. \nFor understanding the variance, we need to understand the \nmeasures of variation.\n– SST = total sum of squares (Total Variation)\n• Measures the variation of the Y i values around their \nmean Y\n– SSR = regression sum of squares (Explained Variation)\n• Variation attributable to the relationship between X and \nY\n– SSE = error sum of squares (Unexplained Variation)\n• Variation in Y attributable to factors other than X\n\nPolynomial Regression\n• This type of regression technique is used to model \nnonlinear equations by taking polynomial functions \nof independent variables.\n• In the figure given below, you can see the red curve \nfits the data better than the green curve. \n• Hence in the situations where the relationship \nbetween the dependent and independent variable \nseems to be non-linear, we can deploy Polynomial \nRegression Models.\n\nPolynomial Regression\n\nLogistic Regression\n• Logistic Regression is also known as Logit, Maximum-Entropy \nclassifier is a supervised learning method for classification. It \nestablishes a relation between dependent class variables and \nindependent variables using regression.\n• The dependent variable is categorical i.e. it can take only integral \nvalues representing different classes. The probabilities \ndescribing the possible outcomes of a query point are modelled \nusing a logistic function. \n• This model belongs to a family of discriminative classifiers. They \nrely on attributes which discriminate the classes well. This model \nis used when we have 2 classes of dependent variables. When \nthere are more than 2 classes, then we have another regression \nmethod which helps us to predict the target variable better.\n\nLinear Discriminant Analysis (LDA)\n• Discriminant Analysis is used for classifying \nobservations to a class or category based on \npredictor (independent) variables of the data.\n• Discriminant Analysis creates a model to predict \nfuture observations where the classes are known. \n• LDA comes to our rescue in situations when logistic \nregression is unstable when\n– Classed are well separated\n– Data is small\n– When we have more than 2 classes\n\nErrors in Linear Regression\n\nRidge Regression\n• A regression model that uses L1 regularization \ntechnique is called Lasso Regression and model \nwhich uses L2 is called Ridge Regression.\n• The key difference between these two is the \npenalty term.\n• Ridge regression adds “squared magnitude” of \ncoefficient as penalty term to the loss function. \nHere the highlighted part represents L2 \nregularization element.\n\nRidge Regression – Cost Function\n• Here, if lambda is zero then you can imagine we get \nback OLS. \n• However, if lambda is very large then it will add too \nmuch weight and it will lead to under-fitting. \n• Having said that it’s important how lambda is \nchosen. This technique works very well to avoid \nover-fitting issue.\n\nLasso Regression – Cost Function\n• Lasso Regression (Least Absolute Shrinkage and \nSelection Operator) adds “absolute value of \nmagnitude” of coefficient as penalty term to the \nloss function.\n• Again, if lambda is zero then we will get back OLS \nwhereas very large value will make coefficients \nzero hence it will under-fit.\n\nComparing \n• The key difference between these techniques is \nthat Lasso shrinks the less important feature’s \ncoefficient to zero thus, removing some feature \naltogether. \n• So, this works well for feature selection in case we \nhave a huge number of features.\n\nElastic Net\n• Elastic net is a popular type of regularized linear \nregression that combines two popular penalties, \nspecifically the L1 and L2 penalty functions.\n• a hyperparameter “alpha” is provided to assign how \nmuch weight is given to each of the L1 and L2 penalties. \n• Alpha is a value between 0 and 1 and is used to weight \nthe contribution of the L1 penalty and one minus the \nalpha value is used to weight the L2 penalty.\n• elastic_net_penalty = (alpha * l1_penalty) + ((1 – alpha) * \nl2_penalty)\n\nRobust Regression\n• A common problem with linear regressions is \ncaused by the presence of outliers. \n• An ordinary least square approach will take them \ninto account and the result (in terms of \ncoefficients) will be therefore biased. \n• In the following figure, there's an example of such a \nbehavior:\n\nRobust Regression\n\nWell-Posed Learning Problems\n• A computer program is said to learn from \nexperience E with respect to some class of tasks \nT and performance measure P, if its \nperformance at tasks in T, as measured by P, \nimproves with experience E.\n\nSimple Regression \n• Let’s say we are given a dataset with the \nfollowing columns (features): how much a \ncompany spends on Radio advertising each year \nand its annual Sales in terms of units sold. \n• We are trying to develop an equation that will \nlet us to predict units sold based on how much a \ncompany spends on radio advertising. \n• The rows (observations) represent companies.\n\nSimple Regression \n\nMaking Predictions\n• Our prediction function outputs an estimate of sales given a company’s \nradio advertising spend and our current values for Weight and Bias.\nSales=Weight Radio+Bias\n⋅\n• Weight\n•     the coefficient for the Radio independent variable. In machine \nlearning we call coefficients weights.\n• Radio\n•     the independent variable. In machine learning we call these variables \nfeatures.\n• Bias\n•     the intercept where our line intercepts the y-axis. In machine learning \nwe can call intercepts bias. Bias offsets all predictions that we make. \n\nMaking Predictions\n• Our algorithm will try to learn the correct values for \nWeight and Bias. By the end of our training, our \nequation will approximate the line of best fit.\n\nCost Function\n• The prediction function is nice, but for our purposes \nwe don’t really need it. What we need is a cost \nfunction so we can start optimizing our weights.\n• Let’s use MSE (L2) as our cost function. MSE \nmeasures the average squared difference between \nan observation’s actual and predicted values. \n• The output is a single number representing the cost, \nor score, associated with our current set of weights. \nOur goal is to minimize MSE to improve the accuracy \nof our model.\n\nMath \n• Given our simple linear equation y=mx+b, we \ncan calculate MSE as:\n\nCode \ndef cost_function(radio, sales, weight, bias):\n    companies = len(radio)\n    total_error = 0.0\n    for i in range(companies):\n        total_error += (sales[i] - (weight*radio[i] + bias))**2\n    return total_error / companies\n\nGradient Descent\n• Gradient descent is an iterative optimization \nalgorithm to find the minimum of a function. \nHere that function is our Loss Function.\n\nUnderstanding Gradient Descent\n\nExample \n• Imagine a valley and a person with no sense of \ndirection who wants to get to the bottom of the \nvalley. \n• He goes down the slope and takes large steps \nwhen the slope is steep and small steps when \nthe slope is less steep. \n• He decides his next position based on his \ncurrent position and stops when he gets to the \nbottom of the valley which was his goal.\n\nExample \n• Let’s try applying gradient descent to m and c \nand approach it step by step:\n– Initially let m = 0 and c = 0. Let L be our \nlearning rate. This controls how much the \nvalue of m changes with each step. L could be \na small value like 0.0001 for good accuracy.\n– Calculate the partial derivative of the loss \nfunction with respect to m, and plug in the \ncurrent values of x, y, m and c in it to obtain \nthe derivative value D.\n\nExample\n• D  is the value of the partial derivative with \nₘ\nrespect to m. Similarly lets find the partial \nderivative with respect to c, Dc :\n\nExample\nNow we update the current value of m and c using \nthe following equation:\n• 4. We repeat this process until our loss function is \na very small value or ideally 0 (which means 0 \nerror or 100% accuracy). The value of m and c that \nwe are left with now will be the optimum values.\n\nExample \n• Now going back to our analogy, m can be considered the \ncurrent position of the person. D is equivalent to the steepness \nof the slope and L can be the speed with which he moves. \n• Now the new value of m that we calculate using the above \nequation will be his next position, and L×D will be the size of \nthe steps he will take. \n• When the slope is more steep (D is more) he takes longer steps \nand when it is less steep (D is less), he takes smaller steps. \nFinally he arrives at the bottom of the valley which corresponds \nto our loss = 0.\n• Now with the optimum value of m and c our model is ready to \nmake predictions !\n\nTypes \n• Batch Gradient Descent\n• Stochastic Gradient Descent\n• Mini Batch gradient descent\n\nBatch Gradient Descent\n• This is a type of gradient descent which processes \nall the training examples for each iteration of \ngradient descent. \n• But if the number of training examples is large, \nthen batch gradient descent is computationally \nvery expensive. \n• Hence if the number of training examples is large, \nthen batch gradient descent is not preferred. \nInstead, we prefer to use stochastic gradient \ndescent or mini-batch gradient descent.\n\nStochastic Gradient Descent\n• This is a type of gradient descent which processes 1 \ntraining example per iteration. \n• Hence, the parameters are being updated even after \none iteration in which only a single example has been \nprocessed. \n• Hence this is quite faster than batch gradient descent. \n• But again, when the number of training examples is \nlarge, even then it processes only one example which \ncan be additional overhead for the system as the \nnumber of iterations will be quite large.\n\nMini Batch Gradient Descent\n• This is a type of gradient descent which works \nfaster than both batch gradient descent and \nstochastic gradient descent. \n• Here b examples where b<m are processed per \niteration. So even if the number of training \nexamples is large, it is processed in batches of b \ntraining examples in one go. \n• Thus, it works for larger training examples and that \ntoo with lesser number of iterations. \n\nTraining \n• Training a model is the process of iteratively improving \nyour prediction equation by looping through the dataset \nmultiple times, each time updating the weight and bias \nvalues in the direction indicated by the slope of the cost \nfunction (gradient). \n• Training is complete when we reach an acceptable error \nthreshold, or when subsequent training iterations fail to \nreduce our cost.\n• Before training we need to initialize our weights (set \ndefault values), set our hyperparameters (learning rate \nand number of iterations), and prepare to log our \nprogress over each iteration.\n\nVisualizing \n\nVisualizing \n\nVisualizing \n\nCost History\n\nMultiple Regression\n• Multiple linear regression is used to estimate the \nrelationship between two or more independent variables \nand one dependent variable. You can use multiple linear \nregression when you want to know:\n• How strong the relationship is between two or more \nindependent variables and one dependent variable (e.g. \nhow rainfall, temperature, and amount of fertilizer \nadded affect crop growth).\n• The value of the dependent variable at a certain value of \nthe independent variables (e.g. the expected yield of a \ncrop at certain levels of rainfall, temperature, and \nfertilizer addition).\n\nMultiple Regression\n• y = the predicted value of the dependent variable\n• B0 = the y-intercept (value of y when all other parameters are set to 0)\n• B1X1= the regression coefficient (B1) of the first independent variable \n(X1) (a.k.a. the effect that increasing the value of the independent \nvariable has on the predicted y value)\n• … = do the same for however many independent variables you are \ntesting\n• BnXn = the regression coefficient of the last independent variable\n• e = model error (a.k.a. how much variation there is in our estimate of \ny)\n\nMultiple Regression\n• Let’s say we are given data on TV, radio, and \nnewspaper advertising spend for a list of \ncompanies, and our goal is to predict sales in \nterms of units sold.\n\nGrowing Complexity\n• As the number of features grows, the complexity of our \nmodel increases and it becomes increasingly difficult to \nvisualize, or even comprehend, our data.\n• One solution is to break the data apart and compare 1-2 \nfeatures at a time. In this example we explore how Radio \nand TV investment impacts Sales.\n\nNormalization \n• Real world dataset contains features that highly vary in \nmagnitudes, units, and range. \n• Normalisation should be performed when the scale of a \nfeature is irrelevant or misleading and not should \nNormalise when the the scale is meaningful.\n• The algorithms which use Euclidean Distance measure are \nsensitive to Magnitudes. Here feature scaling helps to \nweigh all the features equally.\n• Formally, If a feature in the dataset is big in scale compared \nto others then in algorithms where Euclidean distance is \nmeasured this big scaled feature becomes dominating and \nneeds to be normalized. \n\nNormalization \n• Techniques:\n– Feature Scaling or Standardization \n– Min Max Scaling\n– Robust Scaler\n\nFeature Scaling\n• Feature Scaling or Standardization: It is a step of Data \nPre-Processing which is applied to independent \nvariables or features of data. \n• It basically helps to normalise the data within a \nparticular range. Sometimes, it also helps in speeding \nup the calculations in an algorithm.\n• Package Used:\n– sklearn.preprocessing\n• Import:\n– from sklearn.preprocessing import StandardScaler\n\nFeature Scaling\n• Formula used in Backend\n– Standardisation replaces the values by their Z \nscores.\n– Mostly the Fit method is used for Feature \nscaling.\n\nStandard Scaler\n• The idea behind StandardScaler is that it will \ntransform your data such that its distribution \nwill have a mean value 0 and standard deviation \nof 1. \n• Given the distribution of the data, each value in \nthe dataset will have the sample mean value \nsubtracted, and then divided by the standard \ndeviation of the whole dataset.\n\nStandardization\n• Standardization assumes that your data has a \nGaussian (bell curve) distribution. \n• This does not strictly have to be true, but the \ntechnique is more effective if your attribute \ndistribution is Gaussian. \n• Standardization is useful when your data has \nvarying scales and the algorithm you are using does \nmake assumptions about your data having a \nGaussian distribution, such as linear regression, \nlogistic regression, and linear discriminant analysis.\n\nMin-Max Scaling Normalization \n• Here your data Z is rescaled such that any specific z \nwill now be 0 ≤ z ≤ 1, and is done through this formula:\n• It refers to rescaling real valued numeric attributes \ninto the range 0 and 1.\n• It is useful to scale the input attributes for a model \nthat relies on the magnitude of values, such as \ndistance measures used in k-nearest neighbors and in \nthe preparation of coefficients in regression.\n\nMin-Max Scaling Normalization \n• Normalization is a good technique to use when \nyou do not know the distribution of your data or \nwhen you know the distribution is not Gaussian \n(a bell curve). \n• Normalization is useful when your data has \nvarying scales and the algorithm you are using \ndoes not make assumptions about the \ndistribution of your data, such as k-nearest \nneighbors and artificial neural networks.\n\nRobust Scaler\n• Robust Scaler algorithms scale features that are robust to \noutliers. \n• The method it follows is almost similar to the MinMax \nScaler but it uses the interquartile range (rather than the \nmin-max used in MinMax Scaler). \n• The median and scales of the data are removed by this \nscaling algorithm according to the quantile range.\n• It, thus, follows the following formula:\n• Where Q1 is the 1st quartile, and Q3 is the third quartile.\n\nMaking Predictions\n• Our predict function outputs an estimate of \nsales given our current weights (coefficients) \nand a company’s TV, radio, and newspaper \nspend. Our model will try to identify weight \nvalues that most reduce our cost function.\nSales=W1TV+W2Radio+W3Newspaper\n\nInitialize Weights\nW1 = 0.0\nW2 = 0.0\nW3 = 0.0\nweights = np.array([\n    [W1],\n    [W2],\n    [W3]\n])\n\nCode \ndef predict(features, weights):\n  **\n  features - (200, 3)\n  weights - (3, 1)\n  predictions - (200,1)\n  **\n  predictions = np.dot(features, weights)\n  return predictions\n\nCost Function\n• Now we need a cost function to audit how our \nmodel is performing. \n• The math is the same, except we swap the mx+b \nexpression for W1x1+W2x2+W3x3. \n• We also divide the expression by 2 to make \nderivative calculations simpler.\n\nCost Function\ndef cost_function(features, targets, weights):\n    features:(200,3)\n    targets: (200,1)\n    weights:(3,1)\n    returns average squared error among predictions\n    N = len(targets)\n    predictions = predict(features, weights)\n    # Matrix math lets use do this without looping\n    sq_error = (predictions - targets)**2\n    # Return average squared error among predictions\n    return 1.0/(2*N) * sq_error.sum()\n\nGradient descent\n• Again using the Chain rule we can compute the \ngradient–a vector of partial derivatives \ndescribing the slope of the cost function for \neach weight.\nf (W\n′\n1)=−x1(y−(W1x1+W2x2+W3x3))\nf (W\n′\n2)=−x2(y−(W1x1+W2x2+W3x3))\nf (W\n′\n3)=−x3(y−(W1x1+W2x2+W3x3))\n\nGradient descent\ndef update_weights(features, targets, weights, lr):\n    '''\n    Features:(200, 3)      Targets: (200, 1)      Weights:(3, 1)\n    '''\n    predictions = predict(features, weights)\n    #Extract our features\n    x1 = features[:,0]\n    x2 = features[:,1]\n    x3 = features[:,2]\n    # Use matrix cross product (*) to simultaneously\n    # calculate the derivative for each weight\n    d_w1 = -x1*(targets - predictions)\n    d_w2 = -x2*(targets - predictions)\n    d_w3 = -x3*(targets - predictions)\n    # Multiply the mean derivative by the learning rate\n    # and subtract from our weights (remember gradient points in direction of steepest ASCENT)\n    weights[0][0] -= (lr * np.mean(d_w1))\n    weights[1][0] -= (lr * np.mean(d_w2))\n    weights[2][0] -= (lr * np.mean(d_w3))\n    return weights\n\nSimplifying with matrices\n• The gradient descent code above has a lot of duplication. Can \nwe improve it somehow? One way to refactor would be to \nloop through our features and weights–allowing our function \nto handle any number of features. However there is another \neven better technique: vectorized gradient descent.\n• Math\n– We use the same formula as above, but instead of \noperating on a single feature at a time, we use matrix \nmultiplication to operative on all features and weights \nsimultaneously. We replace the xi terms with a single \nfeature matrix X.\ngradient=−X(targets−predictions)\n\nBias Term\n• Our train function is the same as for simple linear \nregression, however we’re going to make one \nfinal tweak before running: add a bias term to \nour feature matrix.\n• In our example, it’s very unlikely that sales would \nbe zero if companies stopped advertising. \n• Possible reasons for this might include past \nadvertising, existing customer relationships, \nretail locations, and salespeople. \n• A bias term will help us capture this base case.\n\nBias Term\n• Code\n– Below we add a constant 1 to our features \nmatrix. By setting this value to 1, it turns our \nbias term into a constant.\nbias = np.ones(shape=(len(features),1))\nfeatures = np.append(bias, features, axis=1)\n\nModel Evaluation\n• After training our model through 1000 \niterations with a learning rate of .0005, we \nfinally arrive at a set of weights we can use to \nmake predictions:\nSales=4.7TV+3.5Radio+.81Newspaper+13.9\n• Our MSE cost dropped from 110.86 to 6.25.\n\nModel Evaluation\n\nUseful web resources\n• www.mitu.co.in \n• www.scikit-learn.org  \n• www.towardsdatascience.com\n• www.medium.com\n• www.analyticsvidhya.com\n• www.kaggle.com\n• www.stephacking.com\n• www.github.com \n\ntushar@tusharkute.com\n      Thank you\nThis presentation is created using LibreOffice Impress 5.1.6.2, can be used freely as per GNU General Public License\nWeb Resources\nhttps://mitu.co.in \nhttp://tusharkute.com\n/mITuSkillologies\n@mitu_group\ncontact@mitu.co.in\n/company/mitu-\nskillologies\nMITUSkillologies",
  "chunks": [
    "Regression Tushar B. Kute, http://tusharkute.com Regression? • Regression analysis is a statistical method that helps us to analyse and understand the relationship between two or more variables of interest. • The process that is adapted to perform regression analysis helps to understand which factors are important, which factors can be ignored and how they are influencing each other. Regression? • Introduction, types of regression. Simple regression- Types, Making predictions, Cost function, Gradient descent, Training, Model evaluation. • Multivariable regression : Growing complexity, Normalization, Making predictions, Initialize weights, Cost function, Gradient descent, Simplifying with matrices, Bias term, Model evaluation Taken From- https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html Regression? • For the regression analysis is be a successful method, we understand the following terms: – Dependent Variable: This is the variable that we are trying to understand or forecast. – Independent",
    "Variable: These are factors that influence the analysis or target variable and provide us with information regarding the relationship of the variables with the target variable. Example: Example: Regression • In regression, we normally have one dependent variable and one or more independent variables. • Here we try to “regress” the value of dependent variable “Y” with the help of the independent variables. • In other words, we are trying to understand, how does the value of ‘Y’ change w.r.t change in ‘X’. Uses of Regression • Regression analysis is used for prediction and forecasting. This has a substantial overlap to the field of machine learning. This statistical method is used across different industries such as, – Financial Industry- Understand the trend in the stock prices, forecast the prices, evaluate risks in the insurance domain – Marketing- Understand the effectiveness of market campaigns, forecast pricing and sales of the product. – Manufacturing- Evaluate the relationship of variables that",
    "determine to define a better engine to provide better performance – Medicine- Forecast the different combination of medicines to prepare generic medicines for diseases. Terminologies • Outliers – Suppose there is an observation in the dataset that has a very high or very low value as compared to the other observations in the data, i.e. it does not belong to the population, such an observation is called an outlier. – In simple words, it is an extreme value. An outlier is a problem because many times it hampers the results we get. Terminologies • Multicollinearity – When the independent variables are highly correlated to each other, then the variables are said to be multicollinear. – Many types of regression techniques assume multicollinearity should not be present in the dataset. – It is because it causes problems in ranking variables based on its importance, or it makes the job difficult in selecting the most important independent variable. Terminologies • Heteroscedasticity – When the variation between the",
    "target variable and the independent variable is not constant, it is called heteroscedasticity. – Example-As one’s income increases, the variability of food consumption will increase. – A poorer person will spend a rather constant amount by always eating inexpensive food; a wealthier person may occasionally buy inexpensive food and at other times, eat expensive meals. – Those with higher incomes display a greater variability of food consumption. Terminologies • When we use unnecessary explanatory variables, it might lead to overfitting. • Overfitting means that our algorithm works well on the training set but is unable to perform better on the test sets. It is also known as a problem of high variance. • When our algorithm works so poorly that it is unable to fit even a training set well, then it is said to underfit the data. It is also known as a problem of high bias. Terminologies Types of Regression • Linear Regression • Multiple Regression • Logistic Regression • Polynomial Regression • Regularized Models",
    "– Ridge Regression – Lasso Regression – ElasticNet Regression • Outlier Based Model – RANSAC Linear Regression • The simplest of all regression types is Linear Regression where it tries to establish relationships between Independent and Dependent variables. • The Dependent variable considered here is always a continuous variable. • Linear Regression is a predictive model used for finding the linear relationship between a dependent variable and one or more independent variables. Linear Regression • Here, ‘Y’ is our dependent variable, which is a continuous numerical and we are trying to understand how does ‘Y’ change with ‘X’. • So, if we are supposed to answer, the above question of “What will be the GRE score of the student, if his CCGPA is 8.32?” our go to option should be linear regression. Simple Linear Regression • As the model is used to predict the dependent variable, the relationship between the variables can be written in the below format. Yi = β0 + β1Xi +εi • Where, – Yi – Dependent variable – β0 —",
    "Intercept – β1 – Slope Coefficient – Xi – Independent Variable – εi – Random Error Term Simple Linear Regression • The main factor that is considered as part of Regression analysis is understanding the variance between the variables. For understanding the variance, we need to understand the measures of variation. – SST = total sum of squares (Total Variation) • Measures the variation of the Y i values around their mean Y – SSR = regression sum of squares (Explained Variation) • Variation attributable to the relationship between X and Y – SSE = error sum of squares (Unexplained Variation) • Variation in Y attributable to factors other than X Polynomial Regression • This type of regression technique is used to model nonlinear equations by taking polynomial functions of independent variables. • In the figure given below, you can see the red curve fits the data better than the green curve. • Hence in the situations where the relationship between the dependent and independent variable seems to be non-linear, we",
    "can deploy Polynomial Regression Models. Polynomial Regression Logistic Regression • Logistic Regression is also known as Logit, Maximum-Entropy classifier is a supervised learning method for classification. It establishes a relation between dependent class variables and independent variables using regression. • The dependent variable is categorical i.e. it can take only integral values representing different classes. The probabilities describing the possible outcomes of a query point are modelled using a logistic function. • This model belongs to a family of discriminative classifiers. They rely on attributes which discriminate the classes well. This model is used when we have 2 classes of dependent variables. When there are more than 2 classes, then we have another regression method which helps us to predict the target variable better. Linear Discriminant Analysis (LDA) • Discriminant Analysis is used for classifying observations to a class or category based on predictor (independent) variables of the",
    "data. • Discriminant Analysis creates a model to predict future observations where the classes are known. • LDA comes to our rescue in situations when logistic regression is unstable when – Classed are well separated – Data is small – When we have more than 2 classes Errors in Linear Regression Ridge Regression • A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. • The key difference between these two is the penalty term. • Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element. Ridge Regression – Cost Function • Here, if lambda is zero then you can imagine we get back OLS. • However, if lambda is very large then it will add too much weight and it will lead to under-fitting. • Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue. Lasso Regression – Cost Function •",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function. • Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit. Comparing • The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. • So, this works well for feature selection in case we have a huge number of features. Elastic Net • Elastic net is a popular type of regularized linear regression that combines two popular penalties, specifically the L1 and L2 penalty functions. • a hyperparameter “alpha” is provided to assign how much weight is given to each of the L1 and L2 penalties. • Alpha is a value between 0 and 1 and is used to weight the contribution of the L1 penalty and one minus the alpha value is used to weight the L2 penalty. • elastic_net_penalty = (alpha * l1_penalty) + ((1 –",
    "alpha) * l2_penalty) Robust Regression • A common problem with linear regressions is caused by the presence of outliers. • An ordinary least square approach will take them into account and the result (in terms of coefficients) will be therefore biased. • In the following figure, there's an example of such a behavior: Robust Regression Well-Posed Learning Problems • A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. Simple Regression • Let’s say we are given a dataset with the following columns (features): how much a company spends on Radio advertising each year and its annual Sales in terms of units sold. • We are trying to develop an equation that will let us to predict units sold based on how much a company spends on radio advertising. • The rows (observations) represent companies. Simple Regression Making Predictions • Our prediction function outputs an",
    "estimate of sales given a company’s radio advertising spend and our current values for Weight and Bias. Sales=Weight Radio+Bias ⋅ • Weight • the coefficient for the Radio independent variable. In machine learning we call coefficients weights. • Radio • the independent variable. In machine learning we call these variables features. • Bias • the intercept where our line intercepts the y-axis. In machine learning we can call intercepts bias. Bias offsets all predictions that we make. Making Predictions • Our algorithm will try to learn the correct values for Weight and Bias. By the end of our training, our equation will approximate the line of best fit. Cost Function • The prediction function is nice, but for our purposes we don’t really need it. What we need is a cost function so we can start optimizing our weights. • Let’s use MSE (L2) as our cost function. MSE measures the average squared difference between an observation’s actual and predicted values. • The output is a single number representing the cost,",
    "or score, associated with our current set of weights. Our goal is to minimize MSE to improve the accuracy of our model. Math • Given our simple linear equation y=mx+b, we can calculate MSE as: Code def cost_function(radio, sales, weight, bias): companies = len(radio) total_error = 0.0 for i in range(companies): total_error += (sales[i] - (weight*radio[i] + bias))**2 return total_error / companies Gradient Descent • Gradient descent is an iterative optimization algorithm to find the minimum of a function. Here that function is our Loss Function. Understanding Gradient Descent Example • Imagine a valley and a person with no sense of direction who wants to get to the bottom of the valley. • He goes down the slope and takes large steps when the slope is steep and small steps when the slope is less steep. • He decides his next position based on his current position and stops when he gets to the bottom of the valley which was his goal. Example • Let’s try applying gradient descent to m and c and approach it step",
    "by step: – Initially let m = 0 and c = 0. Let L be our learning rate. This controls how much the value of m changes with each step. L could be a small value like 0.0001 for good accuracy. – Calculate the partial derivative of the loss function with respect to m, and plug in the current values of x, y, m and c in it to obtain the derivative value D. Example • D is the value of the partial derivative with ₘ respect to m. Similarly lets find the partial derivative with respect to c, Dc : Example Now we update the current value of m and c using the following equation: • 4. We repeat this process until our loss function is a very small value or ideally 0 (which means 0 error or 100% accuracy). The value of m and c that we are left with now will be the optimum values. Example • Now going back to our analogy, m can be considered the current position of the person. D is equivalent to the steepness of the slope and L can be the speed with which he moves. • Now the new value of m that we calculate using the above",
    "equation will be his next position, and L×D will be the size of the steps he will take. • When the slope is more steep (D is more) he takes longer steps and when it is less steep (D is less), he takes smaller steps. Finally he arrives at the bottom of the valley which corresponds to our loss = 0. • Now with the optimum value of m and c our model is ready to make predictions ! Types • Batch Gradient Descent • Stochastic Gradient Descent • Mini Batch gradient descent Batch Gradient Descent • This is a type of gradient descent which processes all the training examples for each iteration of gradient descent. • But if the number of training examples is large, then batch gradient descent is computationally very expensive. • Hence if the number of training examples is large, then batch gradient descent is not preferred. Instead, we prefer to use stochastic gradient descent or mini-batch gradient descent. Stochastic Gradient Descent • This is a type of gradient descent which processes 1 training example per",
    "iteration. • Hence, the parameters are being updated even after one iteration in which only a single example has been processed. • Hence this is quite faster than batch gradient descent. • But again, when the number of training examples is large, even then it processes only one example which can be additional overhead for the system as the number of iterations will be quite large. Mini Batch Gradient Descent • This is a type of gradient descent which works faster than both batch gradient descent and stochastic gradient descent. • Here b examples where b<m are processed per iteration. So even if the number of training examples is large, it is processed in batches of b training examples in one go. • Thus, it works for larger training examples and that too with lesser number of iterations. Training • Training a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weight and bias values in the direction indicated by the slope",
    "of the cost function (gradient). • Training is complete when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost. • Before training we need to initialize our weights (set default values), set our hyperparameters (learning rate and number of iterations), and prepare to log our progress over each iteration. Visualizing Visualizing Visualizing Cost History Multiple Regression • Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable. You can use multiple linear regression when you want to know: • How strong the relationship is between two or more independent variables and one dependent variable (e.g. how rainfall, temperature, and amount of fertilizer added affect crop growth). • The value of the dependent variable at a certain value of the independent variables (e.g. the expected yield of a crop at certain levels of rainfall, temperature, and fertilizer addition). Multiple Regression",
    "• y = the predicted value of the dependent variable • B0 = the y-intercept (value of y when all other parameters are set to 0) • B1X1= the regression coefficient (B1) of the first independent variable (X1) (a.k.a. the effect that increasing the value of the independent variable has on the predicted y value) • … = do the same for however many independent variables you are testing • BnXn = the regression coefficient of the last independent variable • e = model error (a.k.a. how much variation there is in our estimate of y) Multiple Regression • Let’s say we are given data on TV, radio, and newspaper advertising spend for a list of companies, and our goal is to predict sales in terms of units sold. Growing Complexity • As the number of features grows, the complexity of our model increases and it becomes increasingly difficult to visualize, or even comprehend, our data. • One solution is to break the data apart and compare 1-2 features at a time. In this example we explore how Radio and TV investment impacts",
    "Sales. Normalization • Real world dataset contains features that highly vary in magnitudes, units, and range. • Normalisation should be performed when the scale of a feature is irrelevant or misleading and not should Normalise when the the scale is meaningful. • The algorithms which use Euclidean Distance measure are sensitive to Magnitudes. Here feature scaling helps to weigh all the features equally. • Formally, If a feature in the dataset is big in scale compared to others then in algorithms where Euclidean distance is measured this big scaled feature becomes dominating and needs to be normalized. Normalization • Techniques: – Feature Scaling or Standardization – Min Max Scaling – Robust Scaler Feature Scaling • Feature Scaling or Standardization: It is a step of Data Pre-Processing which is applied to independent variables or features of data. • It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm. • Package Used: –",
    "sklearn.preprocessing • Import: – from sklearn.preprocessing import StandardScaler Feature Scaling • Formula used in Backend – Standardisation replaces the values by their Z scores. – Mostly the Fit method is used for Feature scaling. Standard Scaler • The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. • Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset. Standardization • Standardization assumes that your data has a Gaussian (bell curve) distribution. • This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. • Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear",
    "discriminant analysis. Min-Max Scaling Normalization • Here your data Z is rescaled such that any specific z will now be 0 ≤ z ≤ 1, and is done through this formula: • It refers to rescaling real valued numeric attributes into the range 0 and 1. • It is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in k-nearest neighbors and in the preparation of coefficients in regression. Min-Max Scaling Normalization • Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). • Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks. Robust Scaler • Robust Scaler algorithms scale features that are robust to outliers. • The method it follows is almost similar to the MinMax Scaler but it uses the",
    "interquartile range (rather than the min-max used in MinMax Scaler). • The median and scales of the data are removed by this scaling algorithm according to the quantile range. • It, thus, follows the following formula: • Where Q1 is the 1st quartile, and Q3 is the third quartile. Making Predictions • Our predict function outputs an estimate of sales given our current weights (coefficients) and a company’s TV, radio, and newspaper spend. Our model will try to identify weight values that most reduce our cost function. Sales=W1TV+W2Radio+W3Newspaper Initialize Weights W1 = 0.0 W2 = 0.0 W3 = 0.0 weights = np.array([ [W1], [W2], [W3] ]) Code def predict(features, weights): ** features - (200, 3) weights - (3, 1) predictions - (200,1) ** predictions = np.dot(features, weights) return predictions Cost Function • Now we need a cost function to audit how our model is performing. • The math is the same, except we swap the mx+b expression for W1x1+W2x2+W3x3. • We also divide the expression by 2 to make derivative",
    "calculations simpler. Cost Function def cost_function(features, targets, weights): features:(200,3) targets: (200,1) weights:(3,1) returns average squared error among predictions N = len(targets) predictions = predict(features, weights) # Matrix math lets use do this without looping sq_error = (predictions - targets)**2 # Return average squared error among predictions return 1.0/(2*N) * sq_error.sum() Gradient descent • Again using the Chain rule we can compute the gradient–a vector of partial derivatives describing the slope of the cost function for each weight. f (W ′ 1)=−x1(y−(W1x1+W2x2+W3x3)) f (W ′ 2)=−x2(y−(W1x1+W2x2+W3x3)) f (W ′ 3)=−x3(y−(W1x1+W2x2+W3x3)) Gradient descent def update_weights(features, targets, weights, lr): ''' Features:(200, 3) Targets: (200, 1) Weights:(3, 1) ''' predictions = predict(features, weights) #Extract our features x1 = features[:,0] x2 = features[:,1] x3 = features[:,2] # Use matrix cross product (*) to simultaneously # calculate the derivative for each weight d_w1 =",
    "-x1*(targets - predictions) d_w2 = -x2*(targets - predictions) d_w3 = -x3*(targets - predictions) # Multiply the mean derivative by the learning rate # and subtract from our weights (remember gradient points in direction of steepest ASCENT) weights[0][0] -= (lr * np.mean(d_w1)) weights[1][0] -= (lr * np.mean(d_w2)) weights[2][0] -= (lr * np.mean(d_w3)) return weights Simplifying with matrices • The gradient descent code above has a lot of duplication. Can we improve it somehow? One way to refactor would be to loop through our features and weights–allowing our function to handle any number of features. However there is another even better technique: vectorized gradient descent. • Math – We use the same formula as above, but instead of operating on a single feature at a time, we use matrix multiplication to operative on all features and weights simultaneously. We replace the xi terms with a single feature matrix X. gradient=−X(targets−predictions) Bias Term • Our train function is the same as for simple linear",
    "regression, however we’re going to make one final tweak before running: add a bias term to our feature matrix. • In our example, it’s very unlikely that sales would be zero if companies stopped advertising. • Possible reasons for this might include past advertising, existing customer relationships, retail locations, and salespeople. • A bias term will help us capture this base case. Bias Term • Code – Below we add a constant 1 to our features matrix. By setting this value to 1, it turns our bias term into a constant. bias = np.ones(shape=(len(features),1)) features = np.append(bias, features, axis=1) Model Evaluation • After training our model through 1000 iterations with a learning rate of .0005, we finally arrive at a set of weights we can use to make predictions: Sales=4.7TV+3.5Radio+.81Newspaper+13.9 • Our MSE cost dropped from 110.86 to 6.25. Model Evaluation Useful web resources • www.mitu.co.in • www.scikit-learn.org • www.towardsdatascience.com • www.medium.com • www.analyticsvidhya.com •",
    "www.kaggle.com • www.stephacking.com • www.github.com tushar@tusharkute.com Thank you This presentation is created using LibreOffice Impress 5.1.6.2, can be used freely as per GNU General Public License Web Resources https://mitu.co.in http://tusharkute.com /mITuSkillologies @mitu_group contact@mitu.co.in /company/mitu- skillologies MITUSkillologies"
  ],
  "metadata": {
    "filename": "6.-Regression.pdf",
    "processed_date": "2025-06-09T17:24:43.285876",
    "num_chunks": 25
  }
}